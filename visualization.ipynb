{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.gof.cmodule): The same cache key is associated to different modules (/home/lifelogging/.theano/compiledir_Linux-3.19--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.12-64/tmp3OsgSl and /home/lifelogging/.theano/compiledir_Linux-3.19--generic-x86_64-with-debian-jessie-sid-x86_64-2.7.12-64/tmppKht46). This is not supposed to happen! You may need to manually delete your cache directory to fix this.\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5005)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/media/HDD_2TB/marc/Video_Description/egocentric-video-description\")\n",
    "sys.path.append(\"/media/HDD_2TB/marc/multimodal_keras_wrapper\")\n",
    "\n",
    "from viddesc_model import VideoDesc_Model\n",
    "from data_engine.prepare_data import build_dataset\n",
    "from config import load_parameters\n",
    "\n",
    "from keras_wrapper.cnn_model import loadModel, saveModel, transferWeights\n",
    "from keras_wrapper.dataset import loadDataset\n",
    "from keras_wrapper.extra.evaluation import *\n",
    "from keras_wrapper.extra.callbacks import EarlyStopping\n",
    "from keras_wrapper.extra.read_write import dict2pkl\n",
    "\n",
    "# Plot libraries\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg') # run matplotlib without X server (GUI)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image as pilimage\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load basic dataset\n",
    "params = load_parameters()\n",
    "params['REBUILD_DATASET'] = False\n",
    "dataset = build_dataset(params)\n",
    "params['OUTPUT_VOCABULARY_SIZE'] = dataset.vocabulary_len[params['OUTPUTS_IDS_DATASET'][0]]\n",
    "\n",
    "#params['RELOAD'] = 2\n",
    "#params['STORE_PATH'] = 'trained_models/EDUB-SegDesc_features_ArcticVideoCaptionWithInit_txtemb_420_imgemb__lstmenc_600_lstm_484_deepout_maxout_Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lifelogging/code/keras_marc/keras/layers/core.py:656: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, None, 301)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "# Load initial model\n",
    "model = loadModel('/media/HDD_2TB/marc/Video_Description/egocentric-video-description/'+params['STORE_PATH'], \n",
    "                      params['RELOAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "\t\tVideoDesc_Model instance\n",
      "-----------------------------------------------------------------------------------\n",
      "_model_type: TemporallyLinkedVideoDescriptionAtt\n",
      "name: EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test\n",
      "model_path: trained_models/EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test/\n",
      "verbose: 1\n",
      "\n",
      "MODEL params:\n",
      "{'SAMPLE_ON_SETS': ['train', 'val'], 'MAX_OUTPUT_TEXT_LEN': 30, 'SAMPLING_SAVE_MODE': 'list', 'TRG_LAN': 'en', 'SAMPLING': 'max_likelihood', 'SAMPLE_EACH_UPDATES': 50, 'BIDIRECTIONAL_PREV_SENT_ENCODER': True, 'CLIP_C': 10.0, 'TRG_PRETRAINED_VECTORS_TRAINABLE': True, 'USE_PRELU': False, 'ENCODER_HIDDEN_SIZE': 717, 'REBUILD_DATASET': False, 'METRICS': ['coco'], 'OPTIMIZER': 'Adam', 'EVAL_EACH_EPOCHS': False, 'EPOCHS_FOR_SAVE': 1, 'USE_BATCH_NORMALIZATION': True, 'BATCH_SIZE': 64, 'MODEL_NAME': 'EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test', 'BATCH_NORMALIZATION_MODE': 1, 'TRAIN_ON_TRAINVAL': False, 'N_SAMPLES': 5, 'RECURRENT_DROPOUT_P': 0.5, 'PREV_SENT_ENCODER_HIDDEN_SIZE': 484, 'PRE_TRAINED_DATASET_NAME': None, 'MIN_OCCURRENCES_VOCAB': 0, 'OUTPUTS_IDS_DATASET': ['description'], 'INIT_LAYERS': ['tanh'], 'EARLY_STOP': True, 'DATA_AUGMENTATION': False, 'RECURRENT_WEIGHT_DECAY': 0.0, 'ADDITIONAL_OUTPUT_MERGE_MODE': 'sum', 'PARALLEL_LOADERS': 8, 'SAMPLE_WEIGHTS': True, 'FRAMES_COUNTS_FILES': {'test': 'Annotations/%s/test_feat_counts.txt', 'train': 'Annotations/%s/train_feat_counts.txt', 'val': 'Annotations/%s/val_feat_counts.txt'}, 'EVAL_EACH': 50, 'EXTRA_NAME': 'test', 'BIDIRECTIONAL_DEEP_PREV_SENT_ENCODER': True, 'LOSS': 'categorical_crossentropy', 'WRITE_VALID_SAMPLES': True, 'INPUTS_IDS_MODEL': ['video', 'state_below', 'prev_description', 'link_index'], 'NUM_FRAMES': 26, 'LR_GAMMA': 0.95, 'NOISE_AMOUNT': 0.01, 'PRE_TRAINED_MODEL_STORE_PATHS': ['trained_models/MSVD_best_model/', 'trained_models/1BillionWords/'], 'STOP_METRIC': 'Bleu_4', 'FEATURE_NAMES': ['ImageNet'], 'N_LAYERS_ENCODER': 1, 'INPUTS_IDS_DATASET': ['video', 'state_below', 'prev_description', 'link_index'], 'BIDIRECTIONAL_ENCODER': True, 'MAX_OUTPUT_TEXT_LEN_TEST': 50, 'FORCE_RELOAD_VOCABULARY': False, 'TOKENIZATION_METHOD': 'tokenize_icann', 'PRE_TRAINED_MODELS': ['MSVD_best_model', '1BillionWords'], 'OUTPUT_VOCABULARY_SIZE': 15000, 'START_EVAL_ON_EPOCH': 0, 'BEAM_SEARCH': True, 'TARGET_TEXT_EMBEDDING_SIZE': 301, 'DECODER_HIDDEN_SIZE': 484, 'START_SAMPLING_ON_EPOCH': 0, 'LAYERS_MAPPING': [{'initial_memory': 'initial_memory', 'initial_state': 'initial_state', 'bidirectional_encoder': 'bidirectional_encoder_LSTM', 'logit_ctx': 'logit_ctx'}, {'target_text': 'description', 'bidirectional_encoder_LSTM': 'prev_desc_emb_bidirectional_encoder_LSTM', 'target_word_embedding': 'target_word_embedding', 'decoder_AttLSTMCond': 'decoder_AttLSTMCond2Inputs'}], 'MODEL_TYPE': 'TemporallyLinkedVideoDescriptionAtt', 'TRG_PRETRAINED_VECTORS': None, 'PRE_TRAINED_VOCABULARY_NAME': '1BillionWords_vocabulary', 'MAX_EPOCH': 50, 'N_LAYERS_PREV_SENT_ENCODER': 1, 'EVAL_ON_SETS_KERAS': [], 'LINK_SAMPLE_FILES': {'test': 'Annotations/test_link_samples.txt', 'train': 'Annotations/train_link_samples.txt', 'val': 'Annotations/val_link_samples.txt'}, 'SAVE_EACH_EVALUATION': True, 'LOAD_WEIGHTS_ONLY': True, 'NORMALIZE_SAMPLING': True, 'VOCABULARIES_MAPPING': {'prev_description': 'target_text', 'state_below': 'target_text', 'description': 'target_text'}, 'PAD_ON_BATCH': True, 'DESCRIPTION_COUNTS_FILES': {'test': 'Annotations/test_descriptions_counts.npy', 'train': 'Annotations/train_descriptions_counts.npy', 'val': 'Annotations/val_descriptions_counts.npy'}, 'RNN_TYPE': 'LSTM', 'BEAM_SIZE': 10, 'DESCRIPTION_FILES': {'test': 'Annotations/test_descriptions.txt', 'train': 'Annotations/train_descriptions.txt', 'val': 'Annotations/val_descriptions.txt'}, 'LR': 0.001, 'MODE': 'training', 'OPTIMIZED_SEARCH': True, 'CLASSIFIER_ACTIVATION': 'softmax', 'WEIGHTED_MERGE': False, 'FILL': 'end', 'FRAMES_LIST_FILES': {'test': 'Annotations/%s/test_feat_list.txt', 'train': 'Annotations/%s/train_feat_list.txt', 'val': 'Annotations/%s/val_feat_list.txt'}, 'ALPHA_FACTOR': 0.6, 'BEAM_SEARCH_COND_INPUT': 1, 'TEMPERATURE': 1, 'STORE_PATH': 'trained_models/EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test/', 'USE_RECURRENT_DROPOUT': False, 'IMG_EMBEDDING_LAYERS': [], 'DATASET_STORE_PATH': 'datasets/', 'DROPOUT_P': 0.5, 'DATA_ROOT_PATH': '/media/HDD_3TB/DATASETS/EDUB-SegDesc/', 'HOMOGENEOUS_BATCHES': False, 'LR_DECAY': 1, 'DATASET_NAME': 'EDUB-SegDesc_features-linked', 'INPUT_DATA_TYPE': 'video-features', 'USE_DROPOUT': False, 'WEIGHT_DECAY': 0.0001, 'VERBOSE': 1, 'PATIENCE': 10, 'BIDIRECTIONAL_DEEP_ENCODER': True, 'OUTPUTS_IDS_MODEL': ['description'], 'USE_NOISE': True, 'DEEP_OUTPUT_LAYERS': [], 'RELOAD': [2, 0], 'EVAL_ON_SETS': ['val', 'test'], 'IMG_FEAT_SIZE': 1024, 'USE_L2': False}\n",
      "-----------------------------------------------------------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "video (InputLayer)               (None, None, 1024)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_encoder_LSTM (Bidi (None, None, 1434)    9992112     video[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, None, 2458)    0           video[0][0]                      \n",
      "                                                                   bidirectional_encoder_LSTM[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "input_video_gaussian_noise (Gaus (None, None, 2458)    0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_video_batch_normalization  (None, None, 2458)    9832        input_video_gaussian_noise[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "prev_description (InputLayer)    (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "target_word_embedding (Embedding (None, None, 301)     4515000     state_below[0][0]                \n",
      "                                                                   prev_description[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "state_below (InputLayer)         (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_mean (Lambda)             (None, 2458)          0           input_video_batch_normalization[0\n",
      "____________________________________________________________________________________________________\n",
      "prev_desc_emb_bidirectional_enco (None, None, 968)     3043392     target_word_embedding[1][0]      \n",
      "____________________________________________________________________________________________________\n",
      "initial_state (Dense)            (None, 484)           1190156     lambda_mean[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "initial_memory (Dense)           (None, 484)           1190156     lambda_mean[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "target_word_embedding_gaussian_n (None, None, 301)     0           target_word_embedding[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "prev_desc_enc_gaussian_noise (Ga (None, None, 968)     0           prev_desc_emb_bidirectional_encod\n",
      "____________________________________________________________________________________________________\n",
      "initial_state_gaussian_noise (Ga (None, 484)           0           initial_state[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "initial_memory_gaussian_noise (G (None, 484)           0           initial_memory[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "target_word_embedding_batch_norm (None, None, 301)     1204        target_word_embedding_gaussian_no\n",
      "____________________________________________________________________________________________________\n",
      "prev_desc_enc_batch_normalizatio (None, None, 968)     3872        prev_desc_enc_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "initial_state_batch_normalizatio (None, 484)           1936        initial_state_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "initial_memory_batch_normalizati (None, 484)           1936        initial_memory_gaussian_noise[0][\n",
      "____________________________________________________________________________________________________\n",
      "decoder_AttLSTMCond2Inputs (AttL [(None, None, 484), ( 16798258    target_word_embedding_batch_norma\n",
      "                                                                   input_video_batch_normalization[0\n",
      "                                                                   prev_desc_enc_batch_normalization\n",
      "                                                                   initial_state_batch_normalization\n",
      "                                                                   initial_memory_batch_normalizatio\n",
      "____________________________________________________________________________________________________\n",
      "proj_h0_gaussian_noise (Gaussian (None, None, 484)     0           decoder_AttLSTMCond2Inputs[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "proj_h0_batch_normalization (Bat (None, None, 484)     1936        proj_h0_gaussian_noise[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "logit_ctx (TimeDistributed)      multiple              740159      decoder_AttLSTMCond2Inputs[0][1] \n",
      "____________________________________________________________________________________________________\n",
      "logit_prev (TimeDistributed)     multiple              291669      decoder_AttLSTMCond2Inputs[0][3] \n",
      "____________________________________________________________________________________________________\n",
      "logit_lstm (TimeDistributed)     multiple              145985      proj_h0_batch_normalization[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "permutegeneral_1 (PermuteGeneral (None, None, 301)     0           logit_ctx[0][0]                  \n",
      "                                                                   logit_prev[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "logit_emb (TimeDistributed)      multiple              90902       target_word_embedding_batch_norma\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_mlp_gaussian_noise (Ga (None, None, 301)     0           logit_lstm[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_ctx_gaussian_noise (Ga (None, None, 301)     0           permutegeneral_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_emb_gaussian_noise (Ga (None, None, 301)     0           logit_emb[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_prev_gaussian_noise (G (None, None, 301)     0           permutegeneral_1[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_mlp_batch_normalizatio (None, None, 301)     1204        out_layer_mlp_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_ctx_batch_normalizatio (None, None, 301)     1204        out_layer_ctx_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_emb_batch_normalizatio (None, None, 301)     1204        out_layer_emb_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_prev_batch_normalizati (None, None, 301)     1204        out_layer_prev_gaussian_noise[0][\n",
      "____________________________________________________________________________________________________\n",
      "additional_input (Merge)         (None, None, 301)     0           out_layer_mlp_batch_normalization\n",
      "                                                                   out_layer_ctx_batch_normalization\n",
      "                                                                   out_layer_emb_batch_normalization\n",
      "                                                                   out_layer_prev_batch_normalizatio\n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, None, 301)     0           additional_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "description (TimeDistributed)    multiple              4530000     activation_1[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 42,553,321\n",
      "Trainable params: 42,540,555\n",
      "Non-trainable params: 12,766\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "scratch_video_model = VideoDesc_Model(params,\n",
    "                                      type=params['MODEL_TYPE'],\n",
    "                                      verbose=params['VERBOSE'],\n",
    "                                      model_name=params['MODEL_NAME'],\n",
    "                                      vocabularies=dataset.vocabulary,\n",
    "                                      store_path=params['STORE_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "\t\tVideoDesc_Model instance\n",
      "-----------------------------------------------------------------------------------\n",
      "_model_type: TemporallyLinkedVideoDescriptionAtt\n",
      "name: EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test\n",
      "model_path: trained_models/EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test/\n",
      "verbose: 1\n",
      "\n",
      "MODEL params:\n",
      "{'SAMPLE_ON_SETS': ['train', 'val'], 'MAX_OUTPUT_TEXT_LEN': 30, 'SAMPLING_SAVE_MODE': 'list', 'TRG_LAN': 'en', 'SAMPLING': 'max_likelihood', 'SAMPLE_EACH_UPDATES': 50, 'BIDIRECTIONAL_PREV_SENT_ENCODER': True, 'CLIP_C': 10.0, 'TRG_PRETRAINED_VECTORS_TRAINABLE': True, 'USE_PRELU': False, 'ENCODER_HIDDEN_SIZE': 717, 'REBUILD_DATASET': False, 'METRICS': ['coco'], 'OPTIMIZER': 'Adam', 'EVAL_EACH_EPOCHS': False, 'EPOCHS_FOR_SAVE': 1, 'USE_BATCH_NORMALIZATION': True, 'BATCH_SIZE': 64, 'MODEL_NAME': 'EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test', 'BATCH_NORMALIZATION_MODE': 1, 'TRAIN_ON_TRAINVAL': False, 'N_SAMPLES': 5, 'RECURRENT_DROPOUT_P': 0.5, 'PREV_SENT_ENCODER_HIDDEN_SIZE': 717, 'PRE_TRAINED_DATASET_NAME': None, 'MIN_OCCURRENCES_VOCAB': 0, 'OUTPUTS_IDS_DATASET': ['description'], 'INIT_LAYERS': ['tanh'], 'EARLY_STOP': True, 'DATA_AUGMENTATION': False, 'RECURRENT_WEIGHT_DECAY': 0.0, 'ADDITIONAL_OUTPUT_MERGE_MODE': 'sum', 'PARALLEL_LOADERS': 8, 'SAMPLE_WEIGHTS': True, 'FRAMES_COUNTS_FILES': {'test': 'Annotations/%s/test_feat_counts.txt', 'train': 'Annotations/%s/train_feat_counts.txt', 'val': 'Annotations/%s/val_feat_counts.txt'}, 'EVAL_EACH': 50, 'EXTRA_NAME': 'test', 'BIDIRECTIONAL_DEEP_PREV_SENT_ENCODER': True, 'LOSS': 'categorical_crossentropy', 'WRITE_VALID_SAMPLES': True, 'INPUTS_IDS_MODEL': ['video', 'state_below', 'prev_description', 'link_index'], 'NUM_FRAMES': 26, 'LR_GAMMA': 0.95, 'NOISE_AMOUNT': 0.01, 'PRE_TRAINED_MODEL_STORE_PATHS': ['trained_models/MSVD_best_model/', 'trained_models/1BillionWords/'], 'STOP_METRIC': 'Bleu_4', 'FEATURE_NAMES': ['ImageNet'], 'N_LAYERS_ENCODER': 1, 'INPUTS_IDS_DATASET': ['video', 'state_below', 'prev_description', 'link_index'], 'BIDIRECTIONAL_ENCODER': True, 'MAX_OUTPUT_TEXT_LEN_TEST': 50, 'FORCE_RELOAD_VOCABULARY': False, 'TOKENIZATION_METHOD': 'tokenize_icann', 'PRE_TRAINED_MODELS': ['MSVD_best_model', '1BillionWords'], 'OUTPUT_VOCABULARY_SIZE': 15000, 'START_EVAL_ON_EPOCH': 0, 'BEAM_SEARCH': True, 'TARGET_TEXT_EMBEDDING_SIZE': 301, 'DECODER_HIDDEN_SIZE': 484, 'START_SAMPLING_ON_EPOCH': 0, 'LAYERS_MAPPING': [{'initial_memory': 'initial_memory', 'attlstmcond_1': 'decoder_AttLSTMCond2Inputs', 'initial_state': 'initial_state', 'bidirectional_encoder': 'bidirectional_encoder_LSTM', 'logit_ctx': 'logit_ctx'}, {'target_text': 'description', 'bidirectional_encoder_LSTM': 'prev_desc_emb_bidirectional_encoder_LSTM', 'target_word_embedding': 'target_word_embedding', 'decoder_AttLSTMCond': 'decoder_AttLSTMCond2Inputs'}], 'MODEL_TYPE': 'TemporallyLinkedVideoDescriptionAtt', 'TRG_PRETRAINED_VECTORS': None, 'PRE_TRAINED_VOCABULARY_NAME': '1BillionWords_vocabulary', 'MAX_EPOCH': 50, 'N_LAYERS_PREV_SENT_ENCODER': 1, 'EVAL_ON_SETS_KERAS': [], 'LINK_SAMPLE_FILES': {'test': 'Annotations/test_link_samples.txt', 'train': 'Annotations/train_link_samples.txt', 'val': 'Annotations/val_link_samples.txt'}, 'SAVE_EACH_EVALUATION': True, 'LOAD_WEIGHTS_ONLY': True, 'NORMALIZE_SAMPLING': True, 'VOCABULARIES_MAPPING': {'prev_description': 'target_text', 'state_below': 'target_text', 'description': 'target_text'}, 'PAD_ON_BATCH': True, 'DESCRIPTION_COUNTS_FILES': {'test': 'Annotations/test_descriptions_counts.npy', 'train': 'Annotations/train_descriptions_counts.npy', 'val': 'Annotations/val_descriptions_counts.npy'}, 'RNN_TYPE': 'LSTM', 'BEAM_SIZE': 10, 'DESCRIPTION_FILES': {'test': 'Annotations/test_descriptions.txt', 'train': 'Annotations/train_descriptions.txt', 'val': 'Annotations/val_descriptions.txt'}, 'LR': 0.001, 'MODE': 'training', 'OPTIMIZED_SEARCH': True, 'CLASSIFIER_ACTIVATION': 'softmax', 'WEIGHTED_MERGE': False, 'FILL': 'end', 'FRAMES_LIST_FILES': {'test': 'Annotations/%s/test_feat_list.txt', 'train': 'Annotations/%s/train_feat_list.txt', 'val': 'Annotations/%s/val_feat_list.txt'}, 'ALPHA_FACTOR': 0.6, 'BEAM_SEARCH_COND_INPUT': 1, 'TEMPERATURE': 1, 'STORE_PATH': 'trained_models/EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test/', 'USE_RECURRENT_DROPOUT': False, 'IMG_EMBEDDING_LAYERS': [], 'DATASET_STORE_PATH': 'datasets/', 'DROPOUT_P': 0.5, 'DATA_ROOT_PATH': '/media/HDD_3TB/DATASETS/EDUB-SegDesc/', 'HOMOGENEOUS_BATCHES': False, 'LR_DECAY': 1, 'DATASET_NAME': 'EDUB-SegDesc_features-linked', 'INPUT_DATA_TYPE': 'video-features', 'USE_DROPOUT': False, 'WEIGHT_DECAY': 0.0001, 'VERBOSE': 1, 'PATIENCE': 10, 'BIDIRECTIONAL_DEEP_ENCODER': True, 'OUTPUTS_IDS_MODEL': ['description'], 'USE_NOISE': True, 'DEEP_OUTPUT_LAYERS': [], 'RELOAD': [2, 0], 'EVAL_ON_SETS': ['val', 'test'], 'IMG_FEAT_SIZE': 1024, 'USE_L2': False}\n",
      "-----------------------------------------------------------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "video (InputLayer)               (None, None, 1024)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "bidirectional_encoder_LSTM (Bidi (None, None, 1434)    9992112     video[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "merge_1 (Merge)                  (None, None, 2458)    0           video[0][0]                      \n",
      "                                                                   bidirectional_encoder_LSTM[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "input_video_gaussian_noise (Gaus (None, None, 2458)    0           merge_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "input_video_batch_normalization  (None, None, 2458)    9832        input_video_gaussian_noise[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "prev_description (InputLayer)    (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "target_word_embedding (Embedding (None, None, 301)     4515000     state_below[0][0]                \n",
      "                                                                   prev_description[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "state_below (InputLayer)         (None, None)          0                                            \n",
      "____________________________________________________________________________________________________\n",
      "lambda_mean (Lambda)             (None, 2458)          0           input_video_batch_normalization[0\n",
      "____________________________________________________________________________________________________\n",
      "prev_desc_emb_bidirectional_enco (None, None, 1434)    5844984     target_word_embedding[1][0]      \n",
      "____________________________________________________________________________________________________\n",
      "initial_state (Dense)            (None, 484)           1190156     lambda_mean[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "initial_memory (Dense)           (None, 484)           1190156     lambda_mean[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "target_word_embedding_gaussian_n (None, None, 301)     0           target_word_embedding[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "prev_desc_enc_gaussian_noise (Ga (None, None, 1434)    0           prev_desc_emb_bidirectional_encod\n",
      "____________________________________________________________________________________________________\n",
      "initial_state_gaussian_noise (Ga (None, 484)           0           initial_state[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "initial_memory_gaussian_noise (G (None, 484)           0           initial_memory[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "target_word_embedding_batch_norm (None, None, 301)     1204        target_word_embedding_gaussian_no\n",
      "____________________________________________________________________________________________________\n",
      "prev_desc_enc_batch_normalizatio (None, None, 1434)    5736        prev_desc_enc_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "initial_state_batch_normalizatio (None, 484)           1936        initial_state_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "initial_memory_batch_normalizati (None, 484)           1936        initial_memory_gaussian_noise[0][\n",
      "____________________________________________________________________________________________________\n",
      "decoder_AttLSTMCond2Inputs (AttL [(None, None, 484), ( 19046242    target_word_embedding_batch_norma\n",
      "                                                                   input_video_batch_normalization[0\n",
      "                                                                   prev_desc_enc_batch_normalization\n",
      "                                                                   initial_state_batch_normalization\n",
      "                                                                   initial_memory_batch_normalizatio\n",
      "____________________________________________________________________________________________________\n",
      "proj_h0_gaussian_noise (Gaussian (None, None, 484)     0           decoder_AttLSTMCond2Inputs[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "proj_h0_batch_normalization (Bat (None, None, 484)     1936        proj_h0_gaussian_noise[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "logit_ctx (TimeDistributed)      multiple              740159      decoder_AttLSTMCond2Inputs[0][1] \n",
      "____________________________________________________________________________________________________\n",
      "logit_prev (TimeDistributed)     multiple              431935      decoder_AttLSTMCond2Inputs[0][3] \n",
      "____________________________________________________________________________________________________\n",
      "logit_lstm (TimeDistributed)     multiple              145985      proj_h0_batch_normalization[0][0]\n",
      "____________________________________________________________________________________________________\n",
      "permutegeneral_1 (PermuteGeneral (None, None, 301)     0           logit_ctx[0][0]                  \n",
      "                                                                   logit_prev[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "logit_emb (TimeDistributed)      multiple              90902       target_word_embedding_batch_norma\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_mlp_gaussian_noise (Ga (None, None, 301)     0           logit_lstm[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_ctx_gaussian_noise (Ga (None, None, 301)     0           permutegeneral_1[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_emb_gaussian_noise (Ga (None, None, 301)     0           logit_emb[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_prev_gaussian_noise (G (None, None, 301)     0           permutegeneral_1[1][0]           \n",
      "____________________________________________________________________________________________________\n",
      "out_layer_mlp_batch_normalizatio (None, None, 301)     1204        out_layer_mlp_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_ctx_batch_normalizatio (None, None, 301)     1204        out_layer_ctx_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_emb_batch_normalizatio (None, None, 301)     1204        out_layer_emb_gaussian_noise[0][0\n",
      "____________________________________________________________________________________________________\n",
      "out_layer_prev_batch_normalizati (None, None, 301)     1204        out_layer_prev_gaussian_noise[0][\n",
      "____________________________________________________________________________________________________\n",
      "additional_input (Merge)         (None, None, 301)     0           out_layer_mlp_batch_normalization\n",
      "                                                                   out_layer_ctx_batch_normalization\n",
      "                                                                   out_layer_emb_batch_normalization\n",
      "                                                                   out_layer_prev_batch_normalizatio\n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, None, 301)     0           additional_input[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "description (TimeDistributed)    multiple              4530000     activation_1[0][0]               \n",
      "====================================================================================================\n",
      "Total params: 47,745,027\n",
      "Trainable params: 47,731,329\n",
      "Non-trainable params: 13,698\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lifelogging/code/keras_marc/keras/layers/core.py:656: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, None, 301)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'activation_1': [<keras.layers.core.Activation object at 0x7f94cd2a7a50>, 17], u'initial_memory': [<keras.layers.core.Dense object at 0x7f94c8e50ad0>, 7], u'logit_lstm': [<keras.layers.wrappers.TimeDistributed object at 0x7f94c8cd30d0>, 14], u'target_word_embedding': [<keras.layers.embeddings.Embedding object at 0x7f94c8e96110>, 8], u'lambda_1': [<keras.layers.core.Lambda object at 0x7f94cd2a7a90>, 15], u'attlstmcond_1': [<keras.layers.recurrent.AttLSTMCond object at 0x7f94c8f1c6d0>, 11], u'description': [<keras.layers.wrappers.TimeDistributed object at 0x7f94c8cd3150>, 19], u'lambda_mean': [<keras.layers.core.Lambda object at 0x7f94c8df5c10>, 3], u'additional_input': [<keras.engine.topology.Merge object at 0x7f94cd2a7110>, 16], u'dropout_1': [<keras.layers.core.Dropout object at 0x7f94c8df5a10>, 4], u'dropout_3': [<keras.layers.core.Dropout object at 0x7f94c8efe110>, 10], u'dropout_2': [<keras.layers.core.Dropout object at 0x7f94c8e6ac90>, 9], u'video': [<keras.engine.topology.InputLayer object at 0x7f94c8cd31d0>, 0], u'dropout_4': [<keras.layers.core.Dropout object at 0x7f94c8f4bad0>, 12], u'logit_ctx': [<keras.layers.wrappers.TimeDistributed object at 0x7f94c8cd3050>, 13], u'dropout_5': [<keras.layers.core.Dropout object at 0x7f94cd7214d0>, 18], u'state_below': [<keras.engine.topology.InputLayer object at 0x7f94c8e5e150>, 5], u'initial_state': [<keras.layers.core.Dense object at 0x7f94c8e50e50>, 6], u'bidirectional_encoder': [<keras.layers.wrappers.Bidirectional object at 0x7f94ccb2de10>, 1], u'merge_1': [<keras.engine.topology.Merge object at 0x7f94c8e24c50>, 2]}\n",
      "{u'target_word_embedding': [<keras.layers.embeddings.Embedding object at 0x7f94c92a6a90>, 9], u'out_layer_mlp_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94c0b69bd0>, 26], u'out_layer_emb_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94c0b2fe10>, 28], u'source_word_embedding': [<keras.layers.embeddings.Embedding object at 0x7f94ccb89210>, 1], u'additional_input': [<keras.engine.topology.Merge object at 0x7f94ccb89610>, 32], u'maskedmean_1': [<keras.layers.core.MaskedMean object at 0x7f94c92a6a50>, 8], u'state_below_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94ccafd490>, 15], u'initial_memory_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94cc0eef50>, 14], u'target_text': [<keras.layers.wrappers.TimeDistributed object at 0x7f94ccb89410>, 34], u'state_below': [<keras.engine.topology.InputLayer object at 0x7f94ccbb8fd0>, 7], u'initial_state_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94c9200910>, 17], u'permutegeneral_1': [<keras.layers.core.PermuteGeneral object at 0x7f94c0b581d0>, 24], u'out_layer_emb_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94ccb89850>, 31], u'activation_1': [<keras.layers.core.Activation object at 0x7f94c0a29590>, 33], u'annotations_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94cbdf4990>, 6], u'decoder_AttLSTMCond': [<keras.layers.recurrent.AttLSTMCond object at 0x7f94ccb895d0>, 19], u'proj_h0_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94c0c6eb90>, 21], u'src_embedding_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94ccc6d850>, 2], u'annotations_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94ccbb8d90>, 5], u'initial_state': [<keras.layers.core.Dense object at 0x7f94ccbb8550>, 10], u'logit_emb': [<keras.layers.wrappers.TimeDistributed object at 0x7f94ccb89e90>, 25], u'initial_state_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94cc0ee490>, 13], u'logit_lstm': [<keras.layers.wrappers.TimeDistributed object at 0x7f94ccb89350>, 23], u'out_layer_ctx_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94ccb896d0>, 30], u'state_below_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94c8bb2790>, 12], u'out_layer_ctx_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94c0b04590>, 27], u'source_text': [<keras.engine.topology.InputLayer object at 0x7f94ccb89510>, 0], u'masklayer_1': [<keras.layers.core.MaskLayer object at 0x7f94ccb89490>, 16], u'src_embedding_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94cbdcd490>, 3], u'bidirectional_encoder_LSTM': [<keras.layers.wrappers.Bidirectional object at 0x7f94ccbb8d50>, 4], u'initial_memory_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94ccb89550>, 18], u'initial_memory': [<keras.layers.core.Dense object at 0x7f94c8bdfb50>, 11], u'logit_ctx': [<keras.layers.wrappers.TimeDistributed object at 0x7f94ccb89a10>, 22], u'proj_h0_gaussian_noise': [<keras.layers.noise.GaussianNoise object at 0x7f94c8ffdf50>, 20], u'out_layer_mlp_batch_normalization': [<keras.layers.normalization.BatchNormalization object at 0x7f94c0a7cc90>, 29]}\n"
     ]
    }
   ],
   "source": [
    "    ########### Build model\n",
    "    if params['RELOAD'] == 0 or params['LOAD_WEIGHTS_ONLY']: # build new model\n",
    "        video_model = VideoDesc_Model(params,\n",
    "                                      type=params['MODEL_TYPE'],\n",
    "                                      verbose=params['VERBOSE'],\n",
    "                                      model_name=params['MODEL_NAME'],\n",
    "                                      vocabularies=dataset.vocabulary,\n",
    "                                      store_path=params['STORE_PATH'])\n",
    "        dict2pkl(params, params['STORE_PATH'] + '/config')\n",
    "\n",
    "        # Define the inputs and outputs mapping from our Dataset instance to our model\n",
    "        inputMapping = dict()\n",
    "        for i, id_in in enumerate(params['INPUTS_IDS_DATASET']):\n",
    "            if len(video_model.ids_inputs) > i:\n",
    "                pos_source = dataset.ids_inputs.index(id_in)\n",
    "                id_dest = video_model.ids_inputs[i]\n",
    "                inputMapping[id_dest] = pos_source\n",
    "        video_model.setInputsMapping(inputMapping)\n",
    "            \n",
    "        outputMapping = dict()\n",
    "        for i, id_out in enumerate(params['OUTPUTS_IDS_DATASET']):\n",
    "            if len(video_model.ids_outputs) > i:\n",
    "                pos_target = dataset.ids_outputs.index(id_out)\n",
    "                id_dest = video_model.ids_outputs[i]\n",
    "                outputMapping[id_dest] = pos_target\n",
    "        video_model.setOutputsMapping(outputMapping)\n",
    "\n",
    "        # Only load weights from pre-trained model\n",
    "        if params['LOAD_WEIGHTS_ONLY'] and params['RELOAD'] > 0:\n",
    "            for i in range(0, len(params['RELOAD'])):\n",
    "                old_model = loadModel(params['PRE_TRAINED_MODEL_STORE_PATHS'][i], params['RELOAD'][i])\n",
    "                video_model = transferWeights(old_model, video_model, params['LAYERS_MAPPING'][i])\n",
    "            video_model.setOptimizer()\n",
    "            params['RELOAD'] = 0\n",
    "\n",
    "    else: # resume from previously trained model\n",
    "        video_model = loadModel(params['PRE_TRAINED_MODEL_STORE_PATHS'], params['RELOAD'])\n",
    "        video_model.setOptimizer()\n",
    "\n",
    "        if video_model.model_path != params['STORE_PATH']:\n",
    "            video_model.setName(params['MODEL_NAME'], models_path=params['STORE_PATH'], clear_dirs=False)\n",
    "    ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lifelogging/code/keras_marc/keras/layers/core.py:656: UserWarning: `output_shape` argument not specified for layer lambda_1 and cannot be automatically inferred with the Theano backend. Defaulting to output shape `(None, None, 301)` (same as input shape). If the expected output shape is different, specify it via the `output_shape` argument.\n",
      "  .format(self.name, input_shape))\n"
     ]
    }
   ],
   "source": [
    "old_model = loadModel(params['PRE_TRAINED_MODEL_STORE_PATH'], params['RELOAD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print old_model.model_next\n",
    "print old_model.model_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_names = [layer.name for layer in old_model.model.layers]\n",
    "new_names = [layer.name for layer in video_model.model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'video', u'bidirectional_encoder', u'merge_1', u'lambda_mean', u'dropout_1', u'state_below', u'initial_state', u'initial_memory', u'target_word_embedding', u'dropout_2', u'dropout_3', u'attlstmcond_1', u'dropout_4', u'logit_ctx', u'logit_lstm', u'lambda_1', u'additional_input', u'activation_1', u'dropout_5', u'description']\n",
      "\n",
      "['video', 'bidirectional_encoder_LSTM', 'merge_2', 'input_video_gaussian_noise', 'input_video_batch_normalization', 'prev_description', 'target_word_embedding', 'state_below', 'lambda_mean', 'encoder_prev_descLSTM', 'initial_state', 'initial_memory', 'target_word_embedding_gaussian_noise', 'prev_desc_enc_gaussian_noise', 'initial_state_gaussian_noise', 'initial_memory_gaussian_noise', 'target_word_embedding_batch_normalization', 'prev_desc_enc_batch_normalization', 'initial_state_batch_normalization', 'initial_memory_batch_normalization', 'decoder_AttLSTMCond2Inputs', 'proj_h0_gaussian_noise', 'proj_h0_batch_normalization', 'logit_ctx', 'logit_prev', 'logit_lstm', 'permutegeneral_2', 'logit_emb', 'out_layer_mlp_gaussian_noise', 'out_layer_ctx_gaussian_noise', 'out_layer_emb_gaussian_noise', 'out_layer_prev_gaussian_noise', 'out_layer_mlp_batch_normalization', 'out_layer_ctx_batch_normalization', 'out_layer_emb_batch_normalization', 'out_layer_prev_batch_normalization', 'additional_input', 'activation_2', 'maxout_0', 'out_layermaxout_gaussian_noise', 'out_layermaxout_batch_normalization', 'description']\n"
     ]
    }
   ],
   "source": [
    "print old_names\n",
    "print\n",
    "print new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "old_layer_dict = dict([(layer.name, layer) for layer in old_model.model.layers])\n",
    "new_layer_dict = dict([(layer.name, layer) for layer in video_model.model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "(1024, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "\n",
      "(1024, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "(1024, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n"
     ]
    }
   ],
   "source": [
    "old_weights = old_layer_dict['bidirectional_encoder'].get_weights()\n",
    "for w in old_weights:\n",
    "    print w.shape\n",
    "print\n",
    "new_weights = new_layer_dict['bidirectional_encoder_LSTM'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2458, 484)\n",
      "(484,)\n",
      "\n",
      "(2458, 484)\n",
      "(484,)\n"
     ]
    }
   ],
   "source": [
    "old_weights = old_layer_dict['initial_state'].get_weights()\n",
    "for w in old_weights:\n",
    "    print w.shape\n",
    "print\n",
    "new_weights = new_layer_dict['initial_state'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13578, 301)\n",
      "\n",
      "(832, 301)\n"
     ]
    }
   ],
   "source": [
    "old_weights = old_layer_dict['target_word_embedding'].get_weights()\n",
    "for w in old_weights:\n",
    "    print w.shape\n",
    "print\n",
    "new_weights = new_layer_dict['target_word_embedding'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2458,)\n",
      "(484, 2458)\n",
      "(2458, 2458)\n",
      "(2458,)\n",
      "(26,)\n",
      "(301, 1936)\n",
      "(2458, 1936)\n",
      "(484, 1936)\n",
      "(1936,)\n",
      "\n",
      "(2458,)\n",
      "(484, 2458)\n",
      "(2458, 2458)\n",
      "(2458,)\n",
      "()\n",
      "(2458, 1936)\n",
      "(484, 1936)\n",
      "(484, 1936)\n",
      "(301, 1936)\n",
      "(1936,)\n",
      "(484,)\n",
      "(484, 484)\n",
      "(484, 484)\n",
      "(484,)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "old_weights = old_layer_dict['attlstmcond_1'].get_weights()\n",
    "for w in old_weights:\n",
    "    print w.shape\n",
    "print\n",
    "new_weights = new_layer_dict['decoder_AttLSTMCond2Inputs'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2458, 301)\n",
      "(301,)\n",
      "\n",
      "(2458, 301)\n",
      "(301,)\n"
     ]
    }
   ],
   "source": [
    "old_weights = old_layer_dict['logit_ctx'].get_weights()\n",
    "for w in old_weights:\n",
    "    print w.shape\n",
    "print\n",
    "new_weights = new_layer_dict['logit_ctx'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(484, 301)\n",
      "(301,)\n",
      "\n",
      "(484, 301)\n",
      "(301,)\n"
     ]
    }
   ],
   "source": [
    "old_weights = old_layer_dict['logit_lstm'].get_weights()\n",
    "for w in old_weights:\n",
    "    print w.shape\n",
    "print\n",
    "new_weights = new_layer_dict['logit_lstm'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 13578)\n",
      "(13578,)\n",
      "\n",
      "(150, 832)\n",
      "(832,)\n"
     ]
    }
   ],
   "source": [
    "old_weights = old_layer_dict['description'].get_weights()\n",
    "for w in old_weights:\n",
    "    print w.shape\n",
    "print\n",
    "new_weights = new_layer_dict['description'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scratch_video_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-28c46b7f5bd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnext_layer_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_next\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minit_layer_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_init\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mscratch_layer_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mscratch_video_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'scratch_video_model' is not defined"
     ]
    }
   ],
   "source": [
    "new_layer_dict = dict([(layer.name, layer) for layer in video_model.model.layers])\n",
    "next_layer_dict = dict([(layer.name, layer) for layer in video_model.model_next.layers])\n",
    "init_layer_dict = dict([(layer.name, layer) for layer in video_model.model_init.layers])\n",
    "scratch_layer_dict = dict([(layer.name, layer) for layer in scratch_video_model.model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 15000)\n",
      "(15000,)\n",
      "[ 0.36372203  0.09116837 -0.42612514 -0.44709146  0.1938743   0.08794355\n",
      "  0.87447399 -0.40271607  0.26959434 -0.24780366]\n",
      "(301, 15000)\n",
      "(15000,)\n",
      "[ 0.36372203  0.09116837 -0.42612514 -0.44709146  0.1938743   0.08794355\n",
      "  0.87447399 -0.40271607  0.26959434 -0.24780366]\n",
      "(301, 15000)\n",
      "(15000,)\n",
      "[ 0.36372203  0.09116837 -0.42612514 -0.44709146  0.1938743   0.08794355\n",
      "  0.87447399 -0.40271607  0.26959434 -0.24780366]\n"
     ]
    }
   ],
   "source": [
    "#scratch_weights = scratch_layer_dict['description'].get_weights()\n",
    "#for w in scratch_weights:\n",
    "#    print w.shape\n",
    "#print scratch_weights[0][:10,0]\n",
    "\n",
    "new_weights = new_layer_dict['description'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print new_weights[0][:10,0]\n",
    "\n",
    "next_weights = next_layer_dict['description'].get_weights()\n",
    "for w in next_weights:\n",
    "    print w.shape\n",
    "print next_weights[0][:10,0]\n",
    "\n",
    "init_weights = init_layer_dict['description'].get_weights()\n",
    "for w in init_weights:\n",
    "    print w.shape\n",
    "print init_weights[0][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 15000)\n",
      "(15000,)\n",
      "[ 0.36372203  0.09116837 -0.42612514 -0.44709146  0.1938743   0.08794355\n",
      "  0.87447399 -0.40271607  0.26959434 -0.24780366]\n"
     ]
    }
   ],
   "source": [
    "new_weights = new_layer_dict['description'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print new_weights[0][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "[-0.00917163 -0.00153842  0.0106329  -0.00535513 -0.00266077  0.00394826\n",
      "  0.00377789  0.00017624  0.0068661  -0.00624429]\n",
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "[-0.00917163 -0.00153842  0.0106329  -0.00535513 -0.00266077  0.00394826\n",
      "  0.00377789  0.00017624  0.0068661  -0.00624429]\n"
     ]
    }
   ],
   "source": [
    "#scratch_weights = scratch_layer_dict['prev_desc_emb_bidirectional_encoder_LSTM'].get_weights()\n",
    "#for w in scratch_weights:\n",
    "#    print w.shape\n",
    "#print scratch_weights[0][:10,0]\n",
    "\n",
    "new_weights = new_layer_dict['prev_desc_emb_bidirectional_encoder_LSTM'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print new_weights[0][:10,0]\n",
    "\n",
    "#next_weights = next_layer_dict['prev_desc_emb_bidirectional_encoder_LSTM'].get_weights()\n",
    "#for w in next_weights:\n",
    "#    print w.shape\n",
    "#print next_weights[0][:10,0]\n",
    "\n",
    "init_weights = init_layer_dict['prev_desc_emb_bidirectional_encoder_LSTM'].get_weights()\n",
    "for w in init_weights:\n",
    "    print w.shape\n",
    "print init_weights[0][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "[-0.00917163 -0.00153842  0.0106329  -0.00535513 -0.00266077  0.00394826\n",
      "  0.00377789  0.00017624  0.0068661  -0.00624429]\n"
     ]
    }
   ],
   "source": [
    "new_weights = new_layer_dict['prev_desc_emb_bidirectional_encoder_LSTM'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print new_weights[0][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2458,)\n",
      "(484, 2458)\n",
      "(2458, 2458)\n",
      "(2458,)\n",
      "()\n",
      "(2458, 1936)\n",
      "(1434, 1936)\n",
      "(484, 1936)\n",
      "(301, 1936)\n",
      "(1936,)\n",
      "(1434,)\n",
      "(484, 1434)\n",
      "(1434, 1434)\n",
      "(1434,)\n",
      "()\n",
      "\n",
      "(484, 2458)\n",
      "[  5.15654171e-03   5.91200450e-03   7.17863441e-03   1.43885473e-03\n",
      "   7.17661018e-03   7.18622142e-03   7.18453294e-03  -2.62083275e-20\n",
      "   1.75250703e-18  -3.60433587e-05]\n",
      "(2458,)\n",
      "(484, 2458)\n",
      "(2458, 2458)\n",
      "(2458,)\n",
      "()\n",
      "(2458, 1936)\n",
      "(1434, 1936)\n",
      "(484, 1936)\n",
      "(301, 1936)\n",
      "(1936,)\n",
      "(1434,)\n",
      "(484, 1434)\n",
      "(1434, 1434)\n",
      "(1434,)\n",
      "()\n",
      "[  5.15654171e-03   5.91200450e-03   7.17863441e-03   1.43885473e-03\n",
      "   7.17661018e-03   7.18622142e-03   7.18453294e-03  -2.62083275e-20\n",
      "   1.75250703e-18  -3.60433587e-05]\n",
      "(2458,)\n",
      "(484, 2458)\n",
      "(2458, 2458)\n",
      "(2458,)\n",
      "()\n",
      "(2458, 1936)\n",
      "(1434, 1936)\n",
      "(484, 1936)\n",
      "(301, 1936)\n",
      "(1936,)\n",
      "(1434,)\n",
      "(484, 1434)\n",
      "(1434, 1434)\n",
      "(1434,)\n",
      "()\n",
      "[  5.15654171e-03   5.91200450e-03   7.17863441e-03   1.43885473e-03\n",
      "   7.17661018e-03   7.18622142e-03   7.18453294e-03  -2.62083275e-20\n",
      "   1.75250703e-18  -3.60433587e-05]\n"
     ]
    }
   ],
   "source": [
    "#scratch_weights = scratch_layer_dict['description'].get_weights()\n",
    "#for w in scratch_weights:\n",
    "#    print w.shape\n",
    "#print scratch_weights[0][:10,0]\n",
    "\n",
    "new_weights = new_layer_dict['decoder_AttLSTMCond2Inputs'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print\n",
    "print new_weights[1].shape\n",
    "print new_weights[1][:10,0]\n",
    "\n",
    "next_weights = next_layer_dict['decoder_AttLSTMCond2Inputs'].get_weights()\n",
    "for w in next_weights:\n",
    "    print w.shape\n",
    "print next_weights[1][:10,0]\n",
    "\n",
    "init_weights = init_layer_dict['decoder_AttLSTMCond2Inputs'].get_weights()\n",
    "for w in init_weights:\n",
    "    print w.shape\n",
    "print init_weights[1][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abivirnet_model = loadModel(params['PRE_TRAINED_MODEL_STORE_PATHS'][0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abivirnet_layer_dict = dict([(layer.name, layer) for layer in abivirnet_model.model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2458,)\n",
      "(484, 2458)\n",
      "(2458, 2458)\n",
      "(2458,)\n",
      "(26,)\n",
      "(301, 1936)\n",
      "(2458, 1936)\n",
      "(484, 1936)\n",
      "(1936,)\n",
      "\n",
      "(484, 2458)\n",
      "[  5.15654171e-03   5.91200450e-03   7.17863441e-03   1.43885473e-03\n",
      "   7.17661018e-03   7.18622142e-03   7.18453294e-03  -2.62083275e-20\n",
      "   1.75250703e-18  -3.60433587e-05]\n"
     ]
    }
   ],
   "source": [
    "new_weights = abivirnet_layer_dict['attlstmcond_1'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print\n",
    "print new_weights[1].shape\n",
    "print new_weights[1][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2458,)\n",
      "(484, 2458)\n",
      "(2458, 2458)\n",
      "(2458,)\n",
      "(26,)\n",
      "(301, 1936)\n",
      "(2458, 1936)\n",
      "(484, 1936)\n",
      "(1936,)\n"
     ]
    }
   ],
   "source": [
    "w = abivirnet_layer_dict['attlstmcond_1'].get_weights()\n",
    "new_shapes = [w_.shape for w_ in w]\n",
    "for s in new_shapes:\n",
    "    print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words_model = loadModel(params['PRE_TRAINED_MODEL_STORE_PATHS'][1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_layer_dict = dict([(layer.name, layer) for layer in words_model.model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "(301, 2868)\n",
      "(717, 2868)\n",
      "(2868,)\n",
      "[-0.00917163 -0.00153842  0.0106329  -0.00535513 -0.00266077  0.00394826\n",
      "  0.00377789  0.00017624  0.0068661  -0.00624429]\n"
     ]
    }
   ],
   "source": [
    "new_weights = words_layer_dict['bidirectional_encoder_LSTM'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print new_weights[0][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 301)\n",
      "[ 0.00896137  0.0058687   0.02104132  0.01981896  0.03314291 -0.00925888\n",
      " -0.01896602 -0.04003912  0.00936839 -0.02531725]\n",
      "(15000, 301)\n",
      "[ 0.03097132  0.03124981  0.02156269  0.03412048  0.00267495 -0.0386994\n",
      " -0.02263315 -0.03677199  0.02390226  0.0259914 ]\n",
      "(15000, 301)\n",
      "[ 0.03097132  0.03124981  0.02156269  0.03412048  0.00267495 -0.0386994\n",
      " -0.02263315 -0.03677199  0.02390226  0.0259914 ]\n",
      "(15000, 301)\n",
      "[ 0.03097132  0.03124981  0.02156269  0.03412048  0.00267495 -0.0386994\n",
      " -0.02263315 -0.03677199  0.02390226  0.0259914 ]\n"
     ]
    }
   ],
   "source": [
    "scratch_weights = scratch_layer_dict['target_word_embedding'].get_weights()\n",
    "for w in scratch_weights:\n",
    "    print w.shape\n",
    "print scratch_weights[0][:10,0]\n",
    "\n",
    "new_weights = new_layer_dict['target_word_embedding'].get_weights()\n",
    "for w in new_weights:\n",
    "    print w.shape\n",
    "print new_weights[0][:10,0]\n",
    "\n",
    "next_weights = next_layer_dict['target_word_embedding'].get_weights()\n",
    "for w in next_weights:\n",
    "    print w.shape\n",
    "print next_weights[0][:10,0]\n",
    "\n",
    "init_weights = init_layer_dict['target_word_embedding'].get_weights()\n",
    "for w in init_weights:\n",
    "    print w.shape\n",
    "print init_weights[0][:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------\n",
      "\t\tVideoDesc_Model instance\n",
      "-----------------------------------------------------------------------------------\n",
      "_model_type: TemporallyLinkedVideoDescriptionAtt\n",
      "name: EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test\n",
      "model_path: trained_models/EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test/\n",
      "verbose: 1\n",
      "\n",
      "MODEL params:\n",
      "{'SAMPLE_ON_SETS': ['train', 'val'], 'MAX_OUTPUT_TEXT_LEN': 30, 'SAMPLING_SAVE_MODE': 'list', 'TRG_LAN': 'en', 'SAMPLING': 'max_likelihood', 'SAMPLE_EACH_UPDATES': 50, 'BIDIRECTIONAL_PREV_SENT_ENCODER': True, 'CLIP_C': 10.0, 'TRG_PRETRAINED_VECTORS_TRAINABLE': True, 'USE_PRELU': False, 'ENCODER_HIDDEN_SIZE': 717, 'REBUILD_DATASET': False, 'METRICS': ['coco'], 'OPTIMIZER': 'Adam', 'EVAL_EACH_EPOCHS': False, 'EPOCHS_FOR_SAVE': 1, 'USE_BATCH_NORMALIZATION': True, 'BATCH_SIZE': 64, 'MODEL_NAME': 'EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test', 'BATCH_NORMALIZATION_MODE': 1, 'TRAIN_ON_TRAINVAL': False, 'N_SAMPLES': 5, 'RECURRENT_DROPOUT_P': 0.5, 'PREV_SENT_ENCODER_HIDDEN_SIZE': 484, 'PRE_TRAINED_DATASET_NAME': None, 'MIN_OCCURRENCES_VOCAB': 0, 'OUTPUTS_IDS_DATASET': ['description'], 'INIT_LAYERS': ['tanh'], 'EARLY_STOP': True, 'DATA_AUGMENTATION': False, 'RECURRENT_WEIGHT_DECAY': 0.0, 'ADDITIONAL_OUTPUT_MERGE_MODE': 'sum', 'PARALLEL_LOADERS': 8, 'SAMPLE_WEIGHTS': True, 'FRAMES_COUNTS_FILES': {'test': 'Annotations/%s/test_feat_counts.txt', 'train': 'Annotations/%s/train_feat_counts.txt', 'val': 'Annotations/%s/val_feat_counts.txt'}, 'EVAL_EACH': 50, 'EXTRA_NAME': 'test', 'BIDIRECTIONAL_DEEP_PREV_SENT_ENCODER': True, 'LOSS': 'categorical_crossentropy', 'WRITE_VALID_SAMPLES': True, 'INPUTS_IDS_MODEL': ['video', 'state_below', 'prev_description', 'link_index'], 'NUM_FRAMES': 26, 'LR_GAMMA': 0.95, 'NOISE_AMOUNT': 0.01, 'PRE_TRAINED_MODEL_STORE_PATHS': ['trained_models/MSVD_best_model/', 'trained_models/1BillionWords/'], 'STOP_METRIC': 'Bleu_4', 'FEATURE_NAMES': ['ImageNet'], 'N_LAYERS_ENCODER': 1, 'INPUTS_IDS_DATASET': ['video', 'state_below', 'prev_description', 'link_index'], 'BIDIRECTIONAL_ENCODER': True, 'MAX_OUTPUT_TEXT_LEN_TEST': 50, 'FORCE_RELOAD_VOCABULARY': False, 'TOKENIZATION_METHOD': 'tokenize_icann', 'PRE_TRAINED_MODELS': ['MSVD_best_model', '1BillionWords'], 'OUTPUT_VOCABULARY_SIZE': 15000, 'START_EVAL_ON_EPOCH': 0, 'BEAM_SEARCH': True, 'TARGET_TEXT_EMBEDDING_SIZE': 301, 'DECODER_HIDDEN_SIZE': 484, 'START_SAMPLING_ON_EPOCH': 0, 'LAYERS_MAPPING': [{'initial_memory': 'initial_memory', 'initial_state': 'initial_state', 'bidirectional_encoder': 'bidirectional_encoder_LSTM', 'logit_ctx': 'logit_ctx'}, {'target_text': 'description', 'bidirectional_encoder_LSTM': 'prev_desc_emb_bidirectional_encoder_LSTM', 'target_word_embedding': 'target_word_embedding', 'decoder_AttLSTMCond': 'decoder_AttLSTMCond2Inputs'}], 'MODEL_TYPE': 'TemporallyLinkedVideoDescriptionAtt', 'TRG_PRETRAINED_VECTORS': None, 'PRE_TRAINED_VOCABULARY_NAME': '1BillionWords_vocabulary', 'MAX_EPOCH': 50, 'N_LAYERS_PREV_SENT_ENCODER': 1, 'EVAL_ON_SETS_KERAS': [], 'LINK_SAMPLE_FILES': {'test': 'Annotations/test_link_samples.txt', 'train': 'Annotations/train_link_samples.txt', 'val': 'Annotations/val_link_samples.txt'}, 'SAVE_EACH_EVALUATION': True, 'LOAD_WEIGHTS_ONLY': True, 'NORMALIZE_SAMPLING': True, 'VOCABULARIES_MAPPING': {'prev_description': 'target_text', 'state_below': 'target_text', 'description': 'target_text'}, 'PAD_ON_BATCH': True, 'DESCRIPTION_COUNTS_FILES': {'test': 'Annotations/test_descriptions_counts.npy', 'train': 'Annotations/train_descriptions_counts.npy', 'val': 'Annotations/val_descriptions_counts.npy'}, 'RNN_TYPE': 'LSTM', 'BEAM_SIZE': 10, 'DESCRIPTION_FILES': {'test': 'Annotations/test_descriptions.txt', 'train': 'Annotations/train_descriptions.txt', 'val': 'Annotations/val_descriptions.txt'}, 'LR': 0.001, 'MODE': 'training', 'OPTIMIZED_SEARCH': False, 'CLASSIFIER_ACTIVATION': 'softmax', 'WEIGHTED_MERGE': False, 'FILL': 'end', 'FRAMES_LIST_FILES': {'test': 'Annotations/%s/test_feat_list.txt', 'train': 'Annotations/%s/train_feat_list.txt', 'val': 'Annotations/%s/val_feat_list.txt'}, 'ALPHA_FACTOR': 0.6, 'BEAM_SEARCH_COND_INPUT': 1, 'TEMPERATURE': 1, 'STORE_PATH': 'trained_models/EDUB-SegDesc_features-linked_TemporallyLinkedVideoDescriptionAtt_txtemb_301_imgemb__lstmenc_717_lstm_484_additional_output_mode_sum_deepout__Adam_lr_0.001_decay_1-0.95test/', 'USE_RECURRENT_DROPOUT': False, 'IMG_EMBEDDING_LAYERS': [], 'DATASET_STORE_PATH': 'datasets/', 'DROPOUT_P': 0.5, 'DATA_ROOT_PATH': '/media/HDD_3TB/DATASETS/EDUB-SegDesc/', 'HOMOGENEOUS_BATCHES': False, 'LR_DECAY': 1, 'DATASET_NAME': 'EDUB-SegDesc_features-linked', 'INPUT_DATA_TYPE': 'video-features', 'USE_DROPOUT': False, 'WEIGHT_DECAY': 0.0001, 'VERBOSE': 1, 'PATIENCE': 10, 'BIDIRECTIONAL_DEEP_ENCODER': True, 'OUTPUTS_IDS_MODEL': ['description'], 'USE_NOISE': True, 'DEEP_OUTPUT_LAYERS': [], 'RELOAD': 0, 'EVAL_ON_SETS': ['val', 'test'], 'IMG_FEAT_SIZE': 1024, 'USE_L2': False}\n",
      "-----------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras_wrapper.cnn_model import transferWeights as tw\n",
    "\n",
    "video_model = tw(old_model, video_model, params['LAYERS_MAPPING'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'video_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c057fb94bb29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mvideo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabularies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvideo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabularies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'prev_description'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'idx2words'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'video_model' is not defined"
     ]
    }
   ],
   "source": [
    "print video_model.vocabularies.keys()\n",
    "len(video_model.vocabularies['prev_description']['idx2words'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13578"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocabulary['prev_description']['idx2words'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prev_description', 'state_below', 'description']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.vocabulary.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13578"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocabulary['description']['idx2words'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_pretrained = loadDataset(params['DATASET_STORE_PATH']+'Dataset_'+params['PRE_TRAINED_DATASET_NAME']+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prev_description': 13578, 'state_below': 13578, 'description': 13578}\n",
      "{'description': 13578}\n"
     ]
    }
   ],
   "source": [
    "print dataset.vocabulary_len\n",
    "print dataset_pretrained.vocabulary_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.vocabulary['description'] = copy.deepcopy(dataset_pretrained.vocabulary['description'])\n",
    "dataset.vocabulary_len['description'] = copy.deepcopy(dataset_pretrained.vocabulary_len['description'])\n",
    "\n",
    "dataset.vocabulary['prev_description'] = copy.deepcopy(dataset_pretrained.vocabulary['description'])\n",
    "dataset.vocabulary_len['prev_description'] = copy.deepcopy(dataset_pretrained.vocabulary_len['description'])\n",
    "\n",
    "dataset.vocabulary['state_below'] = copy.deepcopy(dataset_pretrained.vocabulary['description'])\n",
    "dataset.vocabulary_len['state_below'] = copy.deepcopy(dataset_pretrained.vocabulary_len['description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description\n",
      "prev_description\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for id_old, id_new in params['VOCABULARIES_MAPPING'].iteritems():\n",
    "            print id_old\n",
    "            print id_new\n",
    "            print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'prev_description'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['VOCABULARIES_MAPPING']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## '-linked' dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[Xdeb, Ydeb] = dataset.getXY_FromIndices('train', range(50), debug=True)\n",
    "[X, Y] = dataset.getXY_FromIndices('train', range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  [-1] --> i looked at the ceiling\n",
      "0 :  [-1] --> i walked in the house\n",
      "0 :  [-1] --> i walked in the house\n",
      "1 : i looked at the ceiling [0] --> i walked on the street\n",
      "1 : i looked at the ceiling [0] --> i walked down the street\n",
      "1 : i looked at the ceiling [0] --> i looked at buildings and trees\n",
      "1 : i walked in the house [0] --> i walked on the street\n",
      "1 : i walked in the house [0] --> i walked down the street\n",
      "1 : i walked in the house [0] --> i looked at buildings and trees\n",
      "1 : i walked in the house [0] --> i walked on the street\n",
      "1 : i walked in the house [0] --> i walked down the street\n",
      "1 : i walked in the house [0] --> i looked at buildings and trees\n",
      "2 : i walked on the street [1] --> i ate and drank with a woman\n",
      "2 : i walked on the street [1] --> i was in a terrace with a woman\n",
      "2 : i walked on the street [1] --> i talked ate and drank with a woman in a terrace\n",
      "2 : i walked down the street [1] --> i ate and drank with a woman\n",
      "2 : i walked down the street [1] --> i was in a terrace with a woman\n",
      "2 : i walked down the street [1] --> i talked ate and drank with a woman in a terrace\n",
      "2 : i looked at buildings and trees [1] --> i ate and drank with a woman\n",
      "2 : i looked at buildings and trees [1] --> i was in a terrace with a woman\n",
      "2 : i looked at buildings and trees [1] --> i talked ate and drank with a woman in a terrace\n",
      "3 : i ate and drank with a woman [2] --> i walked on the street with a woman\n",
      "3 : i ate and drank with a woman [2] --> a woman and i walked on the street and we saw a cathredal\n",
      "3 : i ate and drank with a woman [2] --> a woman and i strolled through the city\n",
      "3 : i was in a terrace with a woman [2] --> i walked on the street with a woman\n",
      "3 : i was in a terrace with a woman [2] --> a woman and i walked on the street and we saw a cathredal\n",
      "3 : i was in a terrace with a woman [2] --> a woman and i strolled through the city\n",
      "3 : i talked ate and drank with a woman in a terrace [2] --> i walked on the street with a woman\n",
      "3 : i talked ate and drank with a woman in a terrace [2] --> a woman and i walked on the street and we saw a cathredal\n",
      "3 : i talked ate and drank with a woman in a terrace [2] --> a woman and i strolled through the city\n",
      "4 : i walked on the street with a woman [3] --> we entered in a shop with paintings stickers and boxes\n",
      "4 : i walked on the street with a woman [3] --> we entered in a shop that sold different things\n",
      "4 : i walked on the street with a woman [3] --> we were on a shop that sold pictures\n",
      "4 : a woman and i walked on the street and we saw a cathredal [3] --> we entered in a shop with paintings stickers and boxes\n",
      "4 : a woman and i walked on the street and we saw a cathredal [3] --> we entered in a shop that sold different things\n",
      "4 : a woman and i walked on the street and we saw a cathredal [3] --> we were on a shop that sold pictures\n",
      "4 : a woman and i strolled through the city [3] --> we entered in a shop with paintings stickers and boxes\n",
      "4 : a woman and i strolled through the city [3] --> we entered in a shop that sold different things\n",
      "4 : a woman and i strolled through the city [3] --> we were on a shop that sold pictures\n",
      "5 : we entered in a shop with paintings stickers and boxes [4] --> i walked down the street\n",
      "5 : we entered in a shop with paintings stickers and boxes [4] --> i was walking on the street\n",
      "5 : we entered in a shop with paintings stickers and boxes [4] --> i saw trees buildings and the sky\n",
      "5 : we entered in a shop that sold different things [4] --> i walked down the street\n",
      "5 : we entered in a shop that sold different things [4] --> i was walking on the street\n",
      "5 : we entered in a shop that sold different things [4] --> i saw trees buildings and the sky\n",
      "5 : we were on a shop that sold pictures [4] --> i walked down the street\n",
      "5 : we were on a shop that sold pictures [4] --> i was walking on the street\n",
      "5 : we were on a shop that sold pictures [4] --> i saw trees buildings and the sky\n",
      "6 : i walked down the street [5] --> i was at a cafeteria with a woman\n",
      "6 : i walked down the street [5] --> i talked with a woman in front of a cafeteria\n"
     ]
    }
   ],
   "source": [
    "for im,sin,sout,l in zip(Xdeb[0],Xdeb[2],Ydeb[0], Xdeb[3]):\n",
    "    print im,':',sin,'['+str(l)+'] -->',sout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 :\n",
      "<unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i looked at the ceiling <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "-1 :\n",
      "<unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "-1 :\n",
      "<unk> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i looked at the ceiling <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i looked at the ceiling <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i looked at the ceiling <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i looked at buildings and trees <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i looked at buildings and trees <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "0 :\n",
      "i walked in the house <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i looked at buildings and trees <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i walked on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i ate and drank with a woman <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i walked on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i was in a terrace with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i walked on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i talked ate and drank with a woman in a terrace <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i ate and drank with a woman <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i was in a terrace with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i talked ate and drank with a woman in a terrace <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i looked at buildings and trees <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i ate and drank with a woman <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i looked at buildings and trees <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i was in a terrace with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "1 :\n",
      "i looked at buildings and trees <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i talked ate and drank with a woman in a terrace <pad> <pad> <pad>\n",
      "------\n",
      "2 :\n",
      "i ate and drank with a woman <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked on the street with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "2 :\n",
      "i ate and drank with a woman <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "a woman and i walked on the street and we saw a <unk> <pad>\n",
      "------\n",
      "2 :\n",
      "i ate and drank with a woman <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "a woman and i strolled through the city <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "2 :\n",
      "i was in a terrace with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked on the street with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "2 :\n",
      "i was in a terrace with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "a woman and i walked on the street and we saw a <unk> <pad>\n",
      "------\n",
      "2 :\n",
      "i was in a terrace with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "a woman and i strolled through the city <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "2 :\n",
      "i talked ate and drank with a woman in a terrace <pad> <pad> <pad>\n",
      "-->\n",
      "i walked on the street with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "2 :\n",
      "i talked ate and drank with a woman in a terrace <pad> <pad> <pad>\n",
      "-->\n",
      "a woman and i walked on the street and we saw a <unk> <pad>\n",
      "------\n",
      "2 :\n",
      "i talked ate and drank with a woman in a terrace <pad> <pad> <pad>\n",
      "-->\n",
      "a woman and i strolled through the city <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "i walked on the street with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "we entered in a shop with <unk> stickers and boxes <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "i walked on the street with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "we entered in a shop that <unk> different things <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "i walked on the street with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "we were on a shop that <unk> pictures <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "a woman and i walked on the street and we saw a <unk> <pad>\n",
      "-->\n",
      "we entered in a shop with <unk> stickers and boxes <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "a woman and i walked on the street and we saw a <unk> <pad>\n",
      "-->\n",
      "we entered in a shop that <unk> different things <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "a woman and i walked on the street and we saw a <unk> <pad>\n",
      "-->\n",
      "we were on a shop that <unk> pictures <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "a woman and i strolled through the city <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "we entered in a shop with <unk> stickers and boxes <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "a woman and i strolled through the city <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "we entered in a shop that <unk> different things <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "3 :\n",
      "a woman and i strolled through the city <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "we were on a shop that <unk> pictures <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we entered in a shop with <unk> stickers and boxes <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we entered in a shop with <unk> stickers and boxes <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i was walking on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we entered in a shop with <unk> stickers and boxes <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i saw trees buildings and the sky <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we entered in a shop that <unk> different things <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we entered in a shop that <unk> different things <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i was walking on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we entered in a shop that <unk> different things <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i saw trees buildings and the sky <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we were on a shop that <unk> pictures <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we were on a shop that <unk> pictures <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i was walking on the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "4 :\n",
      "we were on a shop that <unk> pictures <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i saw trees buildings and the sky <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "5 :\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i was at a cafeteria with a woman <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "------\n",
      "5 :\n",
      "i walked down the street <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "-->\n",
      "i talked with a woman in front of a cafeteria <pad> <pad> <pad> <pad>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "for sin,sout,state_below,l in zip(X[2], Y[0][0], X[1], X[3]):\n",
    "    sin = map(lambda x: dataset.vocabulary['description']['idx2words'][x], sin)\n",
    "    sin = ' '.join(sin)\n",
    "    sout_ = [np.argmax(w) for w in sout]\n",
    "    sout = map(lambda x: dataset.vocabulary['description']['idx2words'][x], sout_)\n",
    "    sout = ' '.join(sout)\n",
    "    print l,':'\n",
    "    print sin\n",
    "    print '-->'\n",
    "    print sout\n",
    "    print '------'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## '-linked-upperbound' dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Xdeb, Ydeb] = dataset.getXY_FromIndices('train', range(50), debug=True)\n",
    "[X, Y] = dataset.getXY_FromIndices('train', range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : i looked at the ceiling --> i looked at the ceiling\n",
      "0 : i looked at the ceiling --> i walked in the house\n",
      "0 : i looked at the ceiling --> i walked in the house\n",
      "0 : i walked in the house --> i looked at the ceiling\n",
      "0 : i walked in the house --> i walked in the house\n",
      "0 : i walked in the house --> i walked in the house\n",
      "0 : i walked in the house --> i looked at the ceiling\n",
      "0 : i walked in the house --> i walked in the house\n",
      "0 : i walked in the house --> i walked in the house\n",
      "1 : i walked on the street --> i walked on the street\n",
      "1 : i walked on the street --> i walked down the street\n",
      "1 : i walked on the street --> i looked at buildings and trees\n",
      "1 : i walked down the street --> i walked on the street\n",
      "1 : i walked down the street --> i walked down the street\n",
      "1 : i walked down the street --> i looked at buildings and trees\n",
      "1 : i looked at buildings and trees --> i walked on the street\n",
      "1 : i looked at buildings and trees --> i walked down the street\n",
      "1 : i looked at buildings and trees --> i looked at buildings and trees\n",
      "2 : i ate and drank with a woman --> i ate and drank with a woman\n",
      "2 : i ate and drank with a woman --> i was in a terrace with a woman\n",
      "2 : i ate and drank with a woman --> i talked ate and drank with a woman in a terrace\n",
      "2 : i was in a terrace with a woman --> i ate and drank with a woman\n",
      "2 : i was in a terrace with a woman --> i was in a terrace with a woman\n",
      "2 : i was in a terrace with a woman --> i talked ate and drank with a woman in a terrace\n",
      "2 : i talked ate and drank with a woman in a terrace --> i ate and drank with a woman\n",
      "2 : i talked ate and drank with a woman in a terrace --> i was in a terrace with a woman\n",
      "2 : i talked ate and drank with a woman in a terrace --> i talked ate and drank with a woman in a terrace\n",
      "3 : i walked on the street with a woman --> i walked on the street with a woman\n",
      "3 : i walked on the street with a woman --> a woman and i walked on the street and we saw a cathredal\n",
      "3 : i walked on the street with a woman --> a woman and i strolled through the city\n",
      "3 : a woman and i walked on the street and we saw a cathredal --> i walked on the street with a woman\n",
      "3 : a woman and i walked on the street and we saw a cathredal --> a woman and i walked on the street and we saw a cathredal\n",
      "3 : a woman and i walked on the street and we saw a cathredal --> a woman and i strolled through the city\n",
      "3 : a woman and i strolled through the city --> i walked on the street with a woman\n",
      "3 : a woman and i strolled through the city --> a woman and i walked on the street and we saw a cathredal\n",
      "3 : a woman and i strolled through the city --> a woman and i strolled through the city\n",
      "4 : we entered in a shop with paintings stickers and boxes --> we entered in a shop with paintings stickers and boxes\n",
      "4 : we entered in a shop with paintings stickers and boxes --> we entered in a shop that sold different things\n",
      "4 : we entered in a shop with paintings stickers and boxes --> we were on a shop that sold pictures\n",
      "4 : we entered in a shop that sold different things --> we entered in a shop with paintings stickers and boxes\n",
      "4 : we entered in a shop that sold different things --> we entered in a shop that sold different things\n",
      "4 : we entered in a shop that sold different things --> we were on a shop that sold pictures\n",
      "4 : we were on a shop that sold pictures --> we entered in a shop with paintings stickers and boxes\n",
      "4 : we were on a shop that sold pictures --> we entered in a shop that sold different things\n",
      "4 : we were on a shop that sold pictures --> we were on a shop that sold pictures\n",
      "5 : i walked down the street --> i walked down the street\n",
      "5 : i walked down the street --> i was walking on the street\n",
      "5 : i walked down the street --> i saw trees buildings and the sky\n",
      "5 : i was walking on the street --> i walked down the street\n",
      "5 : i was walking on the street --> i was walking on the street\n"
     ]
    }
   ],
   "source": [
    "for im,sin,sout in zip(Xdeb[0],Xdeb[2],Ydeb[0]):\n",
    "    print im,':',sin,'-->',sout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## '-linked-upperbound-copy' dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Xdeb, Ydeb] = dataset.getXY_FromIndices('val', range(50), debug=True)\n",
    "[X, Y] = dataset.getXY_FromIndices('val', range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : error --> error\n",
      "1 : i drove my car --> i drove my car\n",
      "2 : i walked on the street --> i walked on the street\n",
      "3 : i entered in the train station and waited for it --> i entered in the train station and waited for it\n",
      "4 : i was inside a train --> i was inside a train\n",
      "5 : i left the station --> i left the station\n",
      "6 : i walked on the street and entered in a building --> i walked on the street and entered in a building\n",
      "7 : i went to my office --> i went to my office\n",
      "8 : i used my laptop --> i used my laptop\n",
      "9 : i went ot the bathroom --> i went ot the bathroom\n",
      "10 : i worked with my laptop --> i worked with my laptop\n",
      "11 : i walked in a building --> i walked in a building\n",
      "12 : i worked at a table with a laptop --> i worked at a table with a laptop\n",
      "13 : i used my laptop and a computer --> i used my laptop and a computer\n",
      "14 : i went to the bathroom --> i went to the bathroom\n",
      "15 : i went to my office --> i went to my office\n",
      "16 : i worked in a library --> i worked in a library\n",
      "17 : i went to my office with my laptop --> i went to my office with my laptop\n",
      "18 : i went to the street and came back to my office --> i went to the street and came back to my office\n",
      "19 : i was at my office with a salad --> i was at my office with a salad\n",
      "20 : error --> error\n",
      "21 : i had lunch with a man and a woman --> i had lunch with a man and a woman\n",
      "22 : i was in an office --> i was in an office\n",
      "23 : i worked on my computer --> i worked on my computer\n",
      "24 : i saw a woman with a laptop --> i saw a woman with a laptop\n",
      "25 : i worked with my laptop --> i worked with my laptop\n",
      "26 : error --> error\n",
      "27 : i used a compute and a laptop --> i used a compute and a laptop\n",
      "28 : error --> error\n",
      "29 : i used a computer and a laptop in an office --> i used a computer and a laptop in an office\n",
      "30 : i walked on the street --> i walked on the street\n",
      "31 : i waited for the train in the train station --> i waited for the train in the train station\n",
      "32 : i was in a train --> i was in a train\n",
      "33 : i walked on the street --> i walked on the street\n",
      "34 : i drove a car --> i drove a car\n",
      "35 : i entered in my home --> i entered in my home\n",
      "36 : i went to the bathroom --> i went to the bathroom\n",
      "37 : i was in the kitchen --> i was in the kitchen\n",
      "38 : i went to use the computer --> i went to use the computer\n",
      "39 : i went to the parking --> i went to the parking\n",
      "40 : i drove a car --> i drove a car\n",
      "41 : i walked on the street --> i walked on the street\n",
      "42 : i talked to a man in a shop --> i talked to a man in a shop\n",
      "43 : i walked on the street --> i walked on the street\n",
      "44 : i drove my car --> i drove my car\n",
      "45 : i went home --> i went home\n",
      "46 : i used my laptop at home --> i used my laptop at home\n",
      "47 : i fed a pet and went to the bathroom --> i fed a pet and went to the bathroom\n",
      "48 : i watched the television --> i watched the television\n",
      "49 : error --> error\n"
     ]
    }
   ],
   "source": [
    "for im,sin,sout in zip(Xdeb[0],Xdeb[2],Ydeb[0]):\n",
    "    print im,':',sin,'-->',sout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## '-linked-upperbound-prev' dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Xdeb, Ydeb] = dataset.getXY_FromIndices('val', range(50), debug=True)\n",
    "[X, Y] = dataset.getXY_FromIndices('val', range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  --> error\n",
      "1 : error --> i drove my car\n",
      "2 : i drove my car --> i walked on the street\n",
      "2 : i used my car --> i walked on the street\n",
      "2 : i travelled by car --> i walked on the street\n",
      "3 : i walked on the street --> i entered in the train station and waited for it\n",
      "3 : i walked in a parking --> i entered in the train station and waited for it\n",
      "3 : i walked in a city --> i entered in the train station and waited for it\n",
      "4 : i entered in the train station and waited for it --> i was inside a train\n",
      "4 : i waited for the train using my mobile --> i was inside a train\n",
      "4 : i used my mobile while i waited for the train in the train station --> i was inside a train\n",
      "5 : i was inside a train --> i left the station\n",
      "5 : i travelled by train --> i left the station\n",
      "5 : i was in a train --> i left the station\n",
      "6 : i left the station --> i walked on the street and entered in a building\n",
      "6 : i walked in the station --> i walked on the street and entered in a building\n",
      "6 : i walked and used automatic stairs --> i walked on the street and entered in a building\n",
      "7 : i walked on the street and entered in a building --> i went to my office\n",
      "7 : i walked on the street --> i went to my office\n",
      "7 : i went to a university --> i went to my office\n",
      "8 : i went to my office --> i used my laptop\n",
      "8 : i entered in my office --> i used my laptop\n",
      "9 : i used my laptop --> i went ot the bathroom\n",
      "9 : i worked with my laptop --> i went ot the bathroom\n",
      "10 : i went ot the bathroom --> i worked with my laptop\n",
      "10 : i walked to the bathroom --> i worked with my laptop\n",
      "11 : i worked with my laptop --> i walked in a building\n",
      "11 : i used my laptop --> i walked in a building\n",
      "11 : i worked on my laptop in the office --> i walked in a building\n",
      "12 : i walked in a building --> i worked at a table with a laptop\n",
      "12 : i walked on a terrace then went to an office --> i worked at a table with a laptop\n",
      "12 : i wento to a terrace and went back to an office --> i worked at a table with a laptop\n",
      "13 : i worked at a table with a laptop --> i used my laptop and a computer\n",
      "13 : i was with a man at a table and we both used the comptuer --> i used my laptop and a computer\n",
      "13 : i used the computer and read a paper at a table --> i used my laptop and a computer\n",
      "14 : i used my laptop and a computer --> i went to the bathroom\n",
      "14 : i worked with my laptop and with a computer --> i went to the bathroom\n",
      "14 : i worked at my office --> i went to the bathroom\n",
      "15 : i went to the bathroom --> i went to my office\n",
      "15 : i used the toilet --> i went to my office\n",
      "15 : i went to hte toilet --> i went to my office\n",
      "16 : i went to my office --> i worked in a library\n",
      "16 : i was at my office --> i worked in a library\n",
      "16 : i was ta mt office and saw a charger --> i worked in a library\n",
      "17 : i worked in a library --> i went to my office with my laptop\n",
      "17 : i worked wiht my computer --> i went to my office with my laptop\n",
      "17 : i was in a library with on aother man working on my computer --> i went to my office with my laptop\n",
      "18 : i went to my office with my laptop --> i went to the street and came back to my office\n",
      "18 : i went to my office to work --> i went to the street and came back to my office\n",
      "18 : i was at my office working --> i went to the street and came back to my office\n"
     ]
    }
   ],
   "source": [
    "for im,sin,sout in zip(Xdeb[0],Xdeb[2],Ydeb[0]):\n",
    "    print im,':',sin,'-->',sout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## '-linked-upperbound-nocopy' dataset inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[Xdeb, Ydeb] = dataset.getXY_FromIndices('val', range(50), debug=True)\n",
    "[X, Y] = dataset.getXY_FromIndices('val', range(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : i drove my car --> i used my car\n",
      "1 : i drove my car --> i drove my car\n",
      "1 : i used my car --> i drove my car\n",
      "1 : i travelled by car --> i walked in a parking\n",
      "2 : i walked on the street --> i walked on the street\n",
      "2 : i walked in a parking --> i walked on the street\n",
      "2 : i walked in a city --> i waited for the train using my mobile\n",
      "3 : i entered in the train station and waited for it --> i entered in the train station and waited for it\n",
      "3 : i waited for the train using my mobile --> i entered in the train station and waited for it\n",
      "3 : i used my mobile while i waited for the train in the train station --> i travelled by train\n",
      "4 : i was inside a train --> i was inside a train\n",
      "4 : i travelled by train --> i was inside a train\n",
      "4 : i was in a train --> i walked in the station\n",
      "5 : i left the station --> i left the station\n",
      "5 : i walked in the station --> i left the station\n",
      "5 : i walked and used automatic stairs --> i walked on the street\n",
      "6 : i walked on the street and entered in a building --> i walked on the street and entered in a building\n",
      "6 : i walked on the street --> i walked on the street and entered in a building\n",
      "6 : i went to a university --> i entered in my office\n",
      "7 : i went to my office --> i went to my office\n",
      "7 : i entered in my office --> i worked with my laptop\n",
      "8 : i used my laptop --> i used my laptop\n",
      "8 : i worked with my laptop --> i walked to the bathroom\n",
      "9 : i went ot the bathroom --> i went ot the bathroom\n",
      "9 : i walked to the bathroom --> i used my laptop\n",
      "10 : i worked with my laptop --> i worked with my laptop\n",
      "10 : i used my laptop --> i worked with my laptop\n",
      "10 : i worked on my laptop in the office --> i walked on a terrace then went to an office\n",
      "11 : i walked in a building --> i walked in a building\n",
      "11 : i walked on a terrace then went to an office --> i walked in a building\n",
      "11 : i wento to a terrace and went back to an office --> i was with a man at a table and we both used the comptuer\n",
      "12 : i worked at a table with a laptop --> i worked at a table with a laptop\n",
      "12 : i was with a man at a table and we both used the comptuer --> i worked at a table with a laptop\n",
      "12 : i used the computer and read a paper at a table --> i worked with my laptop and with a computer\n",
      "13 : i used my laptop and a computer --> i used my laptop and a computer\n",
      "13 : i worked with my laptop and with a computer --> i used my laptop and a computer\n",
      "13 : i worked at my office --> i used the toilet\n",
      "14 : i went to the bathroom --> i went to the bathroom\n",
      "14 : i used the toilet --> i went to the bathroom\n",
      "14 : i went to hte toilet --> i was at my office\n",
      "15 : i went to my office --> i went to my office\n",
      "15 : i was at my office --> i went to my office\n",
      "15 : i was ta mt office and saw a charger --> i worked wiht my computer\n",
      "16 : i worked in a library --> i worked in a library\n",
      "16 : i worked wiht my computer --> i worked in a library\n",
      "16 : i was in a library with on aother man working on my computer --> i went to my office to work\n",
      "17 : i went to my office with my laptop --> i went to my office with my laptop\n",
      "17 : i went to my office to work --> i went to my office with my laptop\n",
      "17 : i was at my office working --> i walked on the street and went to my office\n",
      "18 : i went to the street and came back to my office --> i went to the street and came back to my office\n"
     ]
    }
   ],
   "source": [
    "for im,sin,sout in zip(Xdeb[0],Xdeb[2],Ydeb[0]):\n",
    "    print im,':',sin,'-->',sout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset.vocabulary['description']['idx2words'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
